# Sobub AI

**S**ilence **O**ccasionally **B**roken **U**p **B**y **AI**

A context-aware ambient audio companion that listens to conversations and occasionally plays relevant meme audios based on detected topics.

## Features

- ğŸ¤ Real-time speech recognition using Whisper (local, GPU-accelerated)
- ğŸ·ï¸ Multi-tag system for contextual audio matching
- ğŸ² Configurable random triggering with cooldown
- ğŸ“± Mobile-optimized web interface with automatic network detection
- ğŸ”’ Fully local AI processing (no external APIs)
- ğŸ³ Docker containerized for easy deployment
- ğŸµ Custom audio library management
- ğŸŒ Zero-config mobile connectivity (auto-detects PC backend)

## Architecture

- **Backend**: Python + FastAPI + OpenAI Whisper
- **Frontend**: React + Vite + TailwindCSS
- **Database**: SQLite
- **Deployment**: Docker + Docker Compose with NVIDIA GPU support

## Prerequisites

- Docker & Docker Compose
- NVIDIA GPU with drivers installed
- NVIDIA Container Toolkit ([see GPU_SETUP.md for installation](./GPU_SETUP.md))

## Quick Start

1. Clone the repository:
```bash
git clone https://github.com/badmuriss/sobub-ai.git
cd sobub-ai
```

2. Build and run with Docker Compose:
```bash
docker-compose up --build
```

3. Access the app:
- **From PC**: http://localhost:5173
- **From mobile**: http://YOUR_PC_IP:5173 (automatic backend detection)
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs

## Usage

1. Open the web interface on your phone/device
2. Click the **Start Session** button on the home screen
3. Allow microphone access when prompted
4. Sobub AI will listen and occasionally play relevant meme audios
5. Use the Settings page to:
   - Upload new audio files with tags
   - Adjust cooldown period (default: 5 minutes)
   - Set trigger probability (default: 30%)
   - Manage your audio library

## Project Structure

```
sobub-ai/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ websocket.py
â”‚   â”‚   â”œâ”€â”€ whisper_service.py
â”‚   â”‚   â”œâ”€â”€ context_analyzer.py
â”‚   â”‚   â”œâ”€â”€ meme_manager.py
â”‚   â”‚   â”œâ”€â”€ trigger_engine.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ memes.db
â”‚       â””â”€â”€ audio_files/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.jsx
â”‚       â”œâ”€â”€ components/
â”‚       â””â”€â”€ services/
â””â”€â”€ README.md
```

## Configuration

Settings can be adjusted via the web interface:

- **Cooldown**: Time between audio plays (in seconds)
- **Trigger Probability**: Chance of playing audio when context matches (0-100%)
- **Whisper Model**: Using `base` model (good balance of speed/accuracy)

## How It Works

1. Browser captures microphone audio and sends chunks via WebSocket
2. Whisper transcribes audio in real-time (GPU-accelerated)
3. Context analyzer matches transcribed text against audio tags
4. Trigger engine applies probability + cooldown logic
5. Matched audio is sent back and played through the device speaker

## Development

### Backend Development
```bash
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

### Frontend Development
```bash
cd frontend
npm install
npm run dev
```

## License

MIT

## Contributing

Contributions welcome! Feel free to open issues or submit PRs.

---

**Note**: This project runs entirely locally. No data is sent to external servers.
